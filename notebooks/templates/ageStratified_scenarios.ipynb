{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19: From model prediction to model predictive control\n",
    "\n",
    "## Scenario-analysis with the age-stratified deterministic model\n",
    "\n",
    "*Original code by Ryan S. McGee. Modified by T.W. Alleman in consultation with the BIOMATH research unit headed by prof. Ingmar Nopens.*\n",
    "\n",
    "Copyright (c) 2020 by T.W. Alleman, BIOMATH, Ghent University. All Rights Reserved.\n",
    "\n",
    "This notebook was made to quickly perform scenario analysis with the age-stratified model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:12:49.791967Z",
     "start_time": "2020-06-03T08:12:49.674216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from ipywidgets import interact,fixed,FloatSlider,IntSlider,ToggleButtons\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import scipy\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import scipy.stats as st\n",
    "import networkx # to install networkx in your environment: conda install networkx\n",
    "from covid19model.models import models\n",
    "from covid19model.data import google\n",
    "from covid19model.data import sciensano\n",
    "from covid19model.data import polymod\n",
    "from covid19model.data import parameters\n",
    "# OPTIONAL: Load the \"autoreload\" extension so that package code can change\n",
    "%load_ext autoreload\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load interaction matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T07:45:12.922027Z",
     "start_time": "2020-06-03T07:45:12.692375Z"
    }
   },
   "outputs": [],
   "source": [
    "initN, Nc_home, Nc_work, Nc_schools, Nc_transport, Nc_leisure, Nc_others, Nc_total = polymod.get_interaction_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameter values for age-stratified deterministic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:23:38.965914Z",
     "start_time": "2020-06-03T08:23:38.882665Z"
    }
   },
   "outputs": [],
   "source": [
    "params = parameters.get_agemodel_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:23:39.184700Z",
     "start_time": "2020-06-03T08:23:39.117752Z"
    }
   },
   "outputs": [],
   "source": [
    "params.update({'Nc':Nc_total})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape high-level Sciensano data and Google Community mobility reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:36:23.126977Z",
     "start_time": "2020-06-03T08:36:03.724446Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/miniconda3/envs/work/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "index,startdate,H_tot, ICU_tot, H_in, H_out= sciensano.get_sciensano_data()\n",
    "dates,retail_recreation,grocery,parks,transport,work,residential=google.get_google_mobility_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:09:18.974577Z",
     "start_time": "2020-06-03T08:09:18.919867Z"
    }
   },
   "outputs": [],
   "source": [
    "levels = initN.size\n",
    "initial_states = {'S': initN, 'E': np.ones(levels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:24:46.590506Z",
     "start_time": "2020-06-03T08:24:46.429823Z"
    }
   },
   "outputs": [],
   "source": [
    "model = models.SEIRSAge(initial_states, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First calibrate $\\beta$ and $t_e$ to the NEW hospitalizations during the first days of the pandemic. The new hospitalizations does not depend on the length of hospital stay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rewrite fit functions etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T08:40:29.664318Z",
     "start_time": "2020-06-03T08:40:29.573470Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SEIRSAge' object has no attribute 'LSQ'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-20724f51ca8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# run optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparNames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msetvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpopsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-1e57086179f8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, parNames, positions, bounds, weights, checkpoints, setvar, disp, polish, maxiter, popsize)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#optim_out = scipy.optimize.differential_evolution(self.LSQ, bounds, args=(data,parNames,positions,weights),disp=disp,polish=polish,workers=-1,maxiter=maxiter, popsize=popsize,tol=1e-18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#theta_hat = optim_out.x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     p_hat, obj_fun_val, pars_final_swarm, obj_fun_val_final_swarm = pso.pso(self.LSQ, bounds, args=(data,parNames,positions,weights,checkpoints), swarmsize=popsize, maxiter=maxiter,\n\u001b[0m\u001b[1;32m     31\u001b[0m                                                                                processes=multiprocessing.cpu_count(),minfunc=1e-9, minstep=1e-9,debug=True, particle_output=True)\n\u001b[1;32m     32\u001b[0m     \u001b[0mtheta_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SEIRSAge' object has no attribute 'LSQ'"
     ]
    }
   ],
   "source": [
    "data=[np.transpose(H_in[:,0:7])]\n",
    "# set optimisation settings\n",
    "parNames = ['extraTime','beta'] # must be a list!\n",
    "positions = [np.array([14])] # must be a list!\n",
    "bounds=((20,80),(0.01,0.08)) # must be a list!\n",
    "weights = np.array([1])\n",
    "# run optimisation\n",
    "theta = fit(model,data,parNames,positions,bounds,weights,setvar=True,maxiter=50,popsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "model.plotFit(index[0:7],data,positions,modelClr=['red','orange','blue','yellow'],legendText=('H_in (model)','H_out (model)','H_in (data)','H_out (data)'),titleText='Belgium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calibrate hospitalization lengths from the actual peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "Nc1 = Nc_home+((1-0.20)*Nc_work+(1-0.20)*Nc_transport+0.5*Nc_leisure)\n",
    "Nc2 = 0.6*Nc_home+((1-0.50)*Nc_work+(1-0.50)*Nc_transport)\n",
    "Nc3 = 0.3*Nc_home+sigma*((1-0.70)*Nc_work+(1-0.70)*Nc_transport)\n",
    "\n",
    "# Create a dictionary of past policies\n",
    "chk = {'t':       [model.extraTime+2,model.extraTime+7,model.extraTime+12], \n",
    "       'Nc':      [Nc1,Nc2,Nc3]\n",
    "      }\n",
    "\n",
    "data=[np.transpose(H_in[:,0:50]),np.transpose(H_out[:,0:50])]\n",
    "# set optimisation settings\n",
    "parNames = ['dc'] # must be a list!\n",
    "positions = [np.array([14]),np.array([15])] # must be a list!\n",
    "bounds=[((1,15))] # must be a list!\n",
    "weights = np.array([1,1])\n",
    "# run optimisation\n",
    "#theta = model.fit(data,parNames,positions,bounds,weights,checkpoints=chk,setvar=True,maxiter=30,popsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dc=10.49\n",
    "# plot result\n",
    "model.plotFit(index[0:50],data,positions,checkpoints=chk,modelClr=['red','orange','blue','yellow'],legendText=('H_in (model)','H_out (model)','H_in (data)','H_out (data)'),titleText='Belgium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "Nc1 = Nc_home+((1-0.20)*Nc_work+(1-0.20)*Nc_transport+0.5*Nc_leisure)\n",
    "Nc2 = 0.6*Nc_home+((1-0.50)*Nc_work+(1-0.50)*Nc_transport)\n",
    "Nc3 = 0.3*Nc_home+sigma*((1-0.70)*Nc_work+(1-0.70)*Nc_transport)\n",
    "\n",
    "# Create a dictionary of past policies\n",
    "chk = {'t':       [model.extraTime+2,model.extraTime+7,model.extraTime+13], \n",
    "       'Nc':      [Nc1,Nc2,Nc3]\n",
    "      }\n",
    "\n",
    "data=[np.transpose(H_in[:,0:55]),np.transpose(H_out[:,0:55])]\n",
    "# set optimisation settings\n",
    "parNames = ['dc','dICU'] # must be a list!\n",
    "positions = [np.array([14]),np.array([15])] # must be a list!\n",
    "bounds=((7,12),(6,16)) # must be a list!\n",
    "weights = np.array([1,1])\n",
    "# run optimisation\n",
    "theta = model.fit(data,parNames,positions,bounds,weights,checkpoints=chk,setvar=True,maxiter=10,popsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "model.plotFit(index[0:55],data,positions,checkpoints=chk,modelClr=['red','orange','blue','yellow'],legendText=('H_in (model)','H_out (model)','H_in (data)','H_out (data)'),titleText='Belgium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5\n",
    "Nc1 = Nc_home+((1-0.20)*Nc_work+(1-0.20)*Nc_transport+0.5*Nc_leisure)\n",
    "Nc2 = 0.6*Nc_home+((1-0.50)*Nc_work+(1-0.50)*Nc_transport)\n",
    "Nc3 = 0.3*Nc_home+sigma*((1-0.70)*Nc_work+(1-0.70)*Nc_transport)\n",
    "\n",
    "# Create a dictionary of past policies\n",
    "chk = {'t':       [model.extraTime+2,model.extraTime+7,model.extraTime+13], \n",
    "       'Nc':      [Nc1,Nc2,Nc3]\n",
    "      }\n",
    "\n",
    "data=[np.transpose(H_in[:,0:55]),np.transpose(H_out[:,0:55])]\n",
    "# set optimisation settings\n",
    "parNames = ['dc','dICU','dICUrec'] # must be a list!\n",
    "positions = [np.array([14]),np.array([15])] # must be a list!\n",
    "bounds=((5,11),(10,16),(4,9)) # must be a list!\n",
    "weights = np.array([1,1])\n",
    "# run optimisation\n",
    "theta = model.fit(data,parNames,positions,bounds,weights,checkpoints=chk,setvar=True,maxiter=10,popsize=300)\n",
    "# plot result\n",
    "model.plotFit(index[0:55],data,positions,checkpoints=chk,modelClr=['red','orange','blue','yellow'],legendText=('H_in (model)','H_out (model)','H_in (data)','H_out (data)'),titleText='Belgium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dc = 11.93\n",
    "model.dICU = 9.07\n",
    "model.dICUrec = 7.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.7\n",
    "Nc1 = Nc_home+((1-0.20)*Nc_work+(1-0.20)*Nc_transport+0.5*Nc_leisure)\n",
    "Nc2 = 0.6*Nc_home+((1-0.50)*Nc_work+(1-0.50)*Nc_transport)\n",
    "Nc3 = 0.3*Nc_home+sigma*((1-0.70)*Nc_work+(1-0.70)*Nc_transport)\n",
    "\n",
    "# Create a dictionary of past policies\n",
    "chk = {'t':       [model.extraTime+2,model.extraTime+8,model.extraTime+10], \n",
    "       'Nc':      [Nc1,Nc2,Nc3]\n",
    "      }\n",
    "\n",
    "data=[np.transpose(ICU_tot[:,0:55]),np.transpose(H_tot_cumsum[:,0:55])]\n",
    "# set optimisation settings\n",
    "parNames = ['dc','dICU','dICUrec'] # must be a list!\n",
    "positions = [np.array([6]),np.array([5,6])] # must be a list!\n",
    "bounds=((6,12),(6,10),(4,9)) # must be a list!\n",
    "weights = np.array([9,1])\n",
    "# run optimisation\n",
    "theta = model.fit(data,parNames,positions,bounds,weights,checkpoints=chk,setvar=True,maxiter=10,popsize=100)\n",
    "# plot result\n",
    "model.plotFit(index[0:55],data,positions,checkpoints=chk,modelClr=['red','orange','blue','yellow'],legendText=('H_in (model)','H_out (model)','H_in (data)','H_out (data)'),titleText='Belgium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight underestimation of hospitalizations, most likely due to the fact that residence times in the hospital differ from the values used in our model. Using the UZ Ghent data to estimate distributions and incorporating this uncertainty will surely fix this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from the posterior distribution of beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the high-level python package `pyMC3` is used to sample from the posterior distribution of $\\beta$. However, due to the high level nature of `pyMC3` it is hard to perform an elegant coupling with the BIOMATH covid-19 model. Because the parameter $\\beta$ and the excess time $t_e$ are correlated, we fix the $t_e$ estimate obtained from MLE during the sampling procedure to avoid having a bi- or trimodal distributions for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "from theano.compile.ops import as_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine estimated parameters using MLE\n",
    "model.dc = 11.93\n",
    "model.dICU = 9.07\n",
    "model.dICUrec = 7.49\n",
    "model.extraTime = int(round(37.36))\n",
    "model.beta = 0.03492\n",
    "# Length of dataset\n",
    "n=theano.shared(float(ICU_tot[:,0:55].size))\n",
    "\n",
    "# Define coupling function of pyMC3-ICU \n",
    "@as_op(itypes=[tt.dscalar,tt.dscalar,tt.dscalar,tt.dscalar,tt.dscalar], otypes=[tt.dvector])\n",
    "def coupleICU2COVID19MODEL(beta,dc,dICU,dICUrec,n):\n",
    "    model.beta = beta\n",
    "    model.dc = dc\n",
    "    model.dICU = dICU\n",
    "    model.dICUrec = dICUrec\n",
    "    T = n+model.extraTime-1\n",
    "    model.sim(T,checkpoints=chk)\n",
    "    mdl_out = (model.sumS,model.sumE,model.sumI,model.sumA,model.sumM,model.sumCtot,model.sumICU,model.sumR,model.sumD,model.sumSQ,model.sumEQ,model.sumAQ,model.sumMQ,model.sumRQ,model.sumH_in,model.sumH_out)\n",
    "    positions = np.array([6])\n",
    "    som = 0\n",
    "    for idx in positions:\n",
    "        som = som + np.mean(mdl_out[idx],axis=1).reshape(np.mean(mdl_out[idx],axis=1).size,1)\n",
    "    return som[int(model.extraTime):].flatten()\n",
    "\n",
    "# Define coupling function of pyMC3-Hospital \n",
    "@as_op(itypes=[tt.dscalar,tt.dscalar,tt.dscalar,tt.dscalar,tt.dscalar], otypes=[tt.dvector])\n",
    "def coupleH2COVID19MODEL(beta,dc,dICU,dICUrec,n):\n",
    "    model.beta = beta\n",
    "    model.dc = dc\n",
    "    model.dICU = dICU\n",
    "    model.dICUrec = dICUrec\n",
    "    T = n+model.extraTime-1\n",
    "    model.sim(T,checkpoints=chk)\n",
    "    mdl_out = (model.sumS,model.sumE,model.sumI,model.sumA,model.sumM,model.sumCtot,model.sumICU,model.sumR,model.sumD,model.sumSQ,model.sumEQ,model.sumAQ,model.sumMQ,model.sumRQ,model.sumH_in,model.sumH_out)\n",
    "    positions = np.array([5,6])\n",
    "    som = 0\n",
    "    for idx in positions:\n",
    "        som = som + np.mean(mdl_out[idx],axis=1).reshape(np.mean(mdl_out[idx],axis=1).size,1)\n",
    "    return som[int(model.extraTime):].flatten()\n",
    "\n",
    "# Define prior distributions of parameters\n",
    "with pm.Model() as COVID19MODEL:\n",
    "    db = pm.backends.Text('test')\n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=1.0)\n",
    "    # Priors for unknown model parameters\n",
    "    beta = pm.Normal('beta', mu=model.beta, sigma=0.01)\n",
    "    dc = BoundedNormal('dc', mu=model.dc, sigma=0.1)\n",
    "    dICU = BoundedNormal('dICU', mu=model.dICU, sigma=0.1)\n",
    "    dICUrec = BoundedNormal('dICUrec', mu=model.dICUrec, sigma=0.1)\n",
    "    sigma_ICU = pm.HalfNormal('sigma_ICU', sigma=10)\n",
    "    sigma_H = pm.HalfNormal('sigma_H', sigma=10)\n",
    "\n",
    "    mu_ICU = coupleICU2COVID19MODEL(beta,dc,dICU,dICUrec,n)\n",
    "    mu_H = coupleH2COVID19MODEL(beta,dc,dICU,dICUrec,n)\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    ICU_obs = pm.Normal('ICU_obs', mu=mu_ICU, sigma=sigma_ICU, observed=ICU_tot[:,0:55].flatten())\n",
    "    H_obs = pm.Normal('H_obs', mu=mu_H, sigma=sigma_H, observed=H_tot[:,0:55].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimate = pm.find_MAP(model=COVID19MODEL, method='L-BFGS-B',tol=1e-6)\n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with COVID19MODEL:\n",
    "\n",
    "    # draw 1000 posterior samples\n",
    "    trace = pm.sample(2000,start=map_estimate,step=pm.Slice(),cores=8,trace=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracedict = {\n",
    "    'beta': np.asarray(trace['beta'][-200:]),\n",
    "    'dc': np.asarray(trace['dc'][-200:]),\n",
    "    'dICU': np.asarray(trace['dICU'][-200:]),\n",
    "    'dICUrec': np.asarray(trace['dICUrec'][-200:])\n",
    "}\n",
    "#(pd.DataFrame.from_dict(data=tracedict, orient='index')\n",
    "#   .to_csv('trace.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace,varnames=['beta','dc','dICU','dICUrec'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use the code snippet below to see the correspondence between `'t'` in the `pastPolicy` dictionary and the actual date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data as a list containing data timeseries\n",
    "data=[np.transpose(ICU_tot),np.transpose(H_tot)]\n",
    "print(index[2],index[50],index[57],index[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction matrices of the 2008 study by Mossong were gathered under a business-as-usual scenario. It is thus not possible to use the interaction matrices without doing a correction for social distancing. Even when using only the interactions at home (`Nc_home`), the virus cannot be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stringent measures were taken in the evening of March 17th, which corresponds to time-index 3, however, the measures are only 'enforced' on day 8 in the `pastPolicy` dictionary. Why? The change in human behaviour was very gradual, it took between 10-14 days after March 17th before everyone was working at home (see the Google mobility report). In other words, measures were taken on March 17th, but obedience for these measures was gradual, like a ramp. However, in the model we make a step-wise change. The obedience to measures can be adressed in future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important issue to adress is the home interaction matrix `Nc_home`. All contacts in these matrices are still assumed to be random, during a lockdown, the interactions at home should somehow be corrected for the 'bubble' effect. Since the average household size in belgium is 2 people, I correct the `Nc_home` matrix with a factor 1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.6\n",
    "\n",
    "# Create a dictionary of past policies\n",
    "pastPolicy = {'t':       [2,8,11,50], \n",
    "              'Nc':      [Nc_home+((1-0.20)*Nc_work+(1-0.20)*Nc_transport+0.5*Nc_leisure),\n",
    "                          0.6*Nc_home+((1-0.50)*Nc_work+(1-0.50)*Nc_transport),\n",
    "                          0.3*Nc_home+sigma*((1-0.70)*Nc_work+(1-0.70)*Nc_transport),\n",
    "                          0.3*Nc_home+sigma*((1-0.40)*Nc_work+(1-0.50)*Nc_transport),\n",
    "                          # 40 percent reduction in work --> google covid mobility reports\n",
    "                          # 50 percent reduction in transit --> google covid mobility reports (Contact Koen Schoors for data De Lijn!)\n",
    "                         ]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of future policies\n",
    "futurePolicy = {'t':     [6], # May 21th, June 4th \n",
    "                'Nc':    [0.3*Nc_home+sigma*((1-0.30)*Nc_work+(1-0.40)*Nc_transport+(1-0.50)*Nc_schools)\n",
    "                         ],\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run realTimeScenario\n",
    "model.realTimeScenario(startdate,data,positions,pastPolicy,futurePolicy=futurePolicy,trace=tracedict,T_extra=93,\n",
    "                       modelClr=['red','orange'],legendText=('ICU (model)','Hospital (model)','ICU (data)','Hospital (data)'),\n",
    "                       titleText='Belgium',filename='test.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your figures by altering the variable `filename = xxxxx.svg`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
